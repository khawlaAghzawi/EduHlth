history <- model %>% fit(x = sampling_generator(x_train, y_train, batch_size = batch_size),
epochs = epochs,
steps_per_epoch = round(nrow(x_train) / batch_size),
validation_data = list(x_val, y_val))
# Model evaluation
scores <- model %>% evaluate(x_test, y_test, verbose = 0)
print(scores)
# Perform image segmentation on test images
out <- imageSegmentation(model, x = x_test, threshold = threshold)
# Return the results or do further processing as needed
return(out)
}
# Example usage
dataset_path <- "C:/Users/User/Downloads/Dataset_BUSI_with_GT"
detection_results <- perform_object_detection(dataset_path)
?map
perform_object_detection <- function(dataset_path, dimensions = c(256, 256), test_split = 0.2, validation_split = 0.1, epochs = 20, batch_size = 8, threshold = 0.8) {
# Load and resize images using imager package
img <- list.files(dataset_path, full.names = TRUE, recursive = TRUE) %>%
purrr::map(imager::read.image)
# Remove images with multiple masks
lf <- list.files(wd_resized, full.names = TRUE, recursive = TRUE)
which_extra_mask_1 <- grep("mask_1", lf)
to_remove <- c(sort(c(which_extra_mask_1, which_extra_mask_1 - 1, which_extra_mask_1 - 2)), grep("mask_2", lf))
lf_subset <- lf[-to_remove]
# Load images and masks
img <- loadImages(fileNames = lf_subset, pattern = "mask", patternInclude = FALSE)
mask <- loadImages(fileNames = lf_subset, pattern = "mask", patternInclude = TRUE)
# Split the data
split_tmp <- split_data(n = nrow(img$info), frac_test = test_split, frac_val = validation_split, seed = 123)
test_index <- split_tmp$index_test
val_index <- split_tmp$index_val
train_index <- split_tmp$index_train
# Convert images and masks to 4D arrays for keras
x_train <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = train_index)
y_train <- imagesToKerasInput(images = mask, type = "mask", subset = train_index)
x_test <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = test_index)
y_test <- imagesToKerasInput(images = mask, type = "mask", subset = test_index)
x_val <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = val_index)
y_val <- imagesToKerasInput(images = mask, type = "mask", subset = val_index)
# Create the model architecture
model <- u_net(net_w = dimensions[1], net_h = dimensions[2], grayscale = TRUE, n_class = 1, filters = 32)
# Model compilation
model %>% compile(optimizer = optimizer_adam(), loss = loss_binary_crossentropy, metrics = list(metric_binary_accuracy))
# Set up the sampling generator
sampling_generator <- function(x_data, y_data, batch_size) {
function() {
rows <- sample(1:nrow(x_data), batch_size, replace = TRUE)
list(x_data[rows,,,, drop = FALSE], y_data[rows,,,, drop = FALSE])
}
}
# Train the model
history <- model %>% fit(x = sampling_generator(x_train, y_train, batch_size = batch_size),
epochs = epochs,
steps_per_epoch = round(nrow(x_train) / batch_size),
validation_data = list(x_val, y_val))
# Model evaluation
scores <- model %>% evaluate(x_test, y_test, verbose = 0)
print(scores)
# Perform image segmentation on test images
out <- imageSegmentation(model, x = x_test, threshold = threshold)
# Return the results or do further processing as needed
return(out)
}
# Example usage
dataset_path <- "C:/Users/User/Downloads/Dataset_BUSI_with_GT"
detection_results <- perform_object_detection(dataset_path)
?read.image
perform_object_detection <- function(dataset_path, dimensions = c(256, 256), test_split = 0.2, validation_split = 0.1, epochs = 20, batch_size = 8, threshold = 0.8) {
# Load and resize images using imager package
img <- list.files(dataset_path, full.names = TRUE, recursive = TRUE) %>%
purrr::map(imager::load.image)
# Remove images with multiple masks
lf <- list.files(wd_resized, full.names = TRUE, recursive = TRUE)
which_extra_mask_1 <- grep("mask_1", lf)
to_remove <- c(sort(c(which_extra_mask_1, which_extra_mask_1 - 1, which_extra_mask_1 - 2)), grep("mask_2", lf))
lf_subset <- lf[-to_remove]
# Load images and masks
img <- loadImages(fileNames = lf_subset, pattern = "mask", patternInclude = FALSE)
mask <- loadImages(fileNames = lf_subset, pattern = "mask", patternInclude = TRUE)
# Split the data
split_tmp <- split_data(n = nrow(img$info), frac_test = test_split, frac_val = validation_split, seed = 123)
test_index <- split_tmp$index_test
val_index <- split_tmp$index_val
train_index <- split_tmp$index_train
# Convert images and masks to 4D arrays for keras
x_train <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = train_index)
y_train <- imagesToKerasInput(images = mask, type = "mask", subset = train_index)
x_test <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = test_index)
y_test <- imagesToKerasInput(images = mask, type = "mask", subset = test_index)
x_val <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = val_index)
y_val <- imagesToKerasInput(images = mask, type = "mask", subset = val_index)
# Create the model architecture
model <- u_net(net_w = dimensions[1], net_h = dimensions[2], grayscale = TRUE, n_class = 1, filters = 32)
# Model compilation
model %>% compile(optimizer = optimizer_adam(), loss = loss_binary_crossentropy, metrics = list(metric_binary_accuracy))
# Set up the sampling generator
sampling_generator <- function(x_data, y_data, batch_size) {
function() {
rows <- sample(1:nrow(x_data), batch_size, replace = TRUE)
list(x_data[rows,,,, drop = FALSE], y_data[rows,,,, drop = FALSE])
}
}
# Train the model
history <- model %>% fit(x = sampling_generator(x_train, y_train, batch_size = batch_size),
epochs = epochs,
steps_per_epoch = round(nrow(x_train) / batch_size),
validation_data = list(x_val, y_val))
# Model evaluation
scores <- model %>% evaluate(x_test, y_test, verbose = 0)
print(scores)
# Perform image segmentation on test images
out <- imageSegmentation(model, x = x_test, threshold = threshold)
# Return the results or do further processing as needed
return(out)
}
# Example usage
dataset_path <- "C:/Users/User/Downloads/Dataset_BUSI_with_GT"
detection_results <- perform_object_detection(dataset_path)
dataset_path <- "D:/Testing/imageSegmentation/kaggle/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT"
all_files <- list.files(dataset_path, full.names = TRUE, recursive = TRUE)
# Set the batch size
batch_size <- 50
# Use a loop to load images in batches
img <- vector("list", length = length(all_files))
for (i in seq_along(all_files)) {
img[i] <- load.image(all_files[i])
if (i %% batch_size == 0) {
message("Loaded ", i, " images out of ", length(all_files))
}
}
perform_object_detection <- function(dataset_path, dimensions = c(256, 256), test_split = 0.2, validation_split = 0.1, epochs = 20, batch_size = 8, threshold = 0.8) {
all_files <- list.files(dataset_path, full.names = TRUE, recursive = TRUE)
img <- vector("list", length = length(all_files))
for (i in seq_along(all_files)) {
img[i] <- imager::load.image(all_files[i])
if (i %% batch_size == 0) {
message("Loaded ", i, " images out of ", length(all_files))
}
}
# Remove images with multiple masks
lf <- list.files(wd_resized, full.names = TRUE, recursive = TRUE)
which_extra_mask_1 <- grep("mask_1", lf)
to_remove <- c(sort(c(which_extra_mask_1, which_extra_mask_1 - 1, which_extra_mask_1 - 2)), grep("mask_2", lf))
lf_subset <- lf[-to_remove]
# Load images and masks
img <- loadImages(fileNames = lf_subset, pattern = "mask", patternInclude = FALSE)
mask <- loadImages(fileNames = lf_subset, pattern = "mask", patternInclude = TRUE)
# Split the data
split_tmp <- split_data(n = nrow(img$info), frac_test = test_split, frac_val = validation_split, seed = 123)
test_index <- split_tmp$index_test
val_index <- split_tmp$index_val
train_index <- split_tmp$index_train
# Convert images and masks to 4D arrays for keras
x_train <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = train_index)
y_train <- imagesToKerasInput(images = mask, type = "mask", subset = train_index)
x_test <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = test_index)
y_test <- imagesToKerasInput(images = mask, type = "mask", subset = test_index)
x_val <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = val_index)
y_val <- imagesToKerasInput(images = mask, type = "mask", subset = val_index)
# Create the model architecture
model <- u_net(net_w = dimensions[1], net_h = dimensions[2], grayscale = TRUE, n_class = 1, filters = 32)
# Model compilation
model %>% compile(optimizer = optimizer_adam(), loss = loss_binary_crossentropy, metrics = list(metric_binary_accuracy))
# Set up the sampling generator
sampling_generator <- function(x_data, y_data, batch_size) {
function() {
rows <- sample(1:nrow(x_data), batch_size, replace = TRUE)
list(x_data[rows,,,, drop = FALSE], y_data[rows,,,, drop = FALSE])
}
}
# Train the model
history <- model %>% fit(x = sampling_generator(x_train, y_train, batch_size = batch_size),
epochs = epochs,
steps_per_epoch = round(nrow(x_train) / batch_size),
validation_data = list(x_val, y_val))
# Model evaluation
scores <- model %>% evaluate(x_test, y_test, verbose = 0)
print(scores)
# Perform image segmentation on test images
out <- imageSegmentation(model, x = x_test, threshold = threshold)
# Return the results or do further processing as needed
return(out)
}
# Example usage
dataset_path <- "C:/Users/User/Downloads/Dataset_BUSI_with_GT"
detection_results <- perform_object_detection(dataset_path)
library(imager)
library(purrr)
?load.image
perform_object_detection <- function(dataset_path, dimensions = c(256, 256), test_split = 0.2, validation_split = 0.1, epochs = 20, batch_size = 8, threshold = 0.8) {
all_files <- list.files(dataset_path, full.names = TRUE, recursive = TRUE)
img <- vector("list", length = length(all_files))
for (i in seq_along(all_files)) {
img[i] <- imager::load.image(all_files[i])
if (i %% batch_size == 0) {
message("Loaded ", i, " images out of ", length(all_files))
}
}
# Remove images with multiple masks
lf <- list.files(wd_resized, full.names = TRUE, recursive = TRUE)
which_extra_mask_1 <- grep("mask_1", lf)
to_remove <- c(sort(c(which_extra_mask_1, which_extra_mask_1 - 1, which_extra_mask_1 - 2)), grep("mask_2", lf))
lf_subset <- lf[-to_remove]
# Load images and masks
img <- loadImages(fileNames = lf_subset, pattern = "mask", patternInclude = FALSE)
mask <- loadImages(fileNames = lf_subset, pattern = "mask", patternInclude = TRUE)
# Split the data
split_tmp <- split_data(n = nrow(img$info), frac_test = test_split, frac_val = validation_split, seed = 123)
test_index <- split_tmp$index_test
val_index <- split_tmp$index_val
train_index <- split_tmp$index_train
# Convert images and masks to 4D arrays for keras
x_train <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = train_index)
y_train <- imagesToKerasInput(images = mask, type = "mask", subset = train_index)
x_test <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = test_index)
y_test <- imagesToKerasInput(images = mask, type = "mask", subset = test_index)
x_val <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = val_index)
y_val <- imagesToKerasInput(images = mask, type = "mask", subset = val_index)
# Create the model architecture
model <- u_net(net_w = dimensions[1], net_h = dimensions[2], grayscale = TRUE, n_class = 1, filters = 32)
# Model compilation
model %>% compile(optimizer = optimizer_adam(), loss = loss_binary_crossentropy, metrics = list(metric_binary_accuracy))
# Set up the sampling generator
sampling_generator <- function(x_data, y_data, batch_size) {
function() {
rows <- sample(1:nrow(x_data), batch_size, replace = TRUE)
list(x_data[rows,,,, drop = FALSE], y_data[rows,,,, drop = FALSE])
}
}
# Train the model
history <- model %>% fit(x = sampling_generator(x_train, y_train, batch_size = batch_size),
epochs = epochs,
steps_per_epoch = round(nrow(x_train) / batch_size),
validation_data = list(x_val, y_val))
# Model evaluation
scores <- model %>% evaluate(x_test, y_test, verbose = 0)
print(scores)
# Perform image segmentation on test images
out <- imageSegmentation(model, x = x_test, threshold = threshold)
# Return the results or do further processing as needed
return(out)
}
# Example usage
dataset_path <- "C:/Users/User/Downloads/Dataset_BUSI_with_GT"
detection_results <- perform_object_detection(dataset_path)
perform_object_detection <- function(dataset_path, dimensions = c(256, 256), test_split = 0.2, validation_split = 0.1, epochs = 20, batch_size = 8, threshold = 0.8) {
all_files <- list.files(dataset_path, full.names = TRUE, recursive = TRUE)
img <- vector("list", length = length(all_files))
for (i in seq_along(all_files)) {
img[i] <- imager::load.image(all_files[i])
if (i %% batch_size == 0) {
message("Loaded ", i, " images out of ", length(all_files))
}
}
# Remove images with multiple masks
lf <- list.files(wd_resized, full.names = TRUE, recursive = TRUE)
which_extra_mask_1 <- grep("mask_1", lf)
to_remove <- c(sort(c(which_extra_mask_1, which_extra_mask_1 - 1, which_extra_mask_1 - 2)), grep("mask_2", lf))
lf_subset <- lf[-to_remove]
# Load images and masks
img <- imager::load.image(fileNames = lf_subset, pattern = "mask", patternInclude = FALSE)
mask <- imager::load.image(fileNames = lf_subset, pattern = "mask", patternInclude = TRUE)
# Split the data
split_tmp <- split_data(n = nrow(img$info), frac_test = test_split, frac_val = validation_split, seed = 123)
test_index <- split_tmp$index_test
val_index <- split_tmp$index_val
train_index <- split_tmp$index_train
# Convert images and masks to 4D arrays for keras
x_train <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = train_index)
y_train <- imagesToKerasInput(images = mask, type = "mask", subset = train_index)
x_test <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = test_index)
y_test <- imagesToKerasInput(images = mask, type = "mask", subset = test_index)
x_val <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = val_index)
y_val <- imagesToKerasInput(images = mask, type = "mask", subset = val_index)
# Create the model architecture
model <- u_net(net_w = dimensions[1], net_h = dimensions[2], grayscale = TRUE, n_class = 1, filters = 32)
# Model compilation
model %>% compile(optimizer = optimizer_adam(), loss = loss_binary_crossentropy, metrics = list(metric_binary_accuracy))
# Set up the sampling generator
sampling_generator <- function(x_data, y_data, batch_size) {
function() {
rows <- sample(1:nrow(x_data), batch_size, replace = TRUE)
list(x_data[rows,,,, drop = FALSE], y_data[rows,,,, drop = FALSE])
}
}
# Train the model
history <- model %>% fit(x = sampling_generator(x_train, y_train, batch_size = batch_size),
epochs = epochs,
steps_per_epoch = round(nrow(x_train) / batch_size),
validation_data = list(x_val, y_val))
# Model evaluation
scores <- model %>% evaluate(x_test, y_test, verbose = 0)
print(scores)
# Perform image segmentation on test images
out <- imageSegmentation(model, x = x_test, threshold = threshold)
# Return the results or do further processing as needed
return(out)
}
# Example usage
dataset_path <- "C:/Users/User/Downloads/Dataset_BUSI_with_GT"
detection_results <- perform_object_detection(dataset_path)
perform_object_detection <- function(dataset_path, dimensions = c(256, 256), test_split = 0.2, validation_split = 0.1, epochs = 20, batch_size = 8, threshold = 0.8) {
all_files <- list.files(dataset_path, full.names = TRUE, recursive = TRUE)
img <- vector("list", length = length(all_files))
for (i in seq_along(all_files)) {
img[i] <- imager::load.image(all_files[i])
if (i %% batch_size == 0) {
message("Loaded ", i, " images out of ", length(all_files))
}
}
# Remove images with multiple masks
lf <- list.files(dataset_path, full.names = TRUE, recursive = TRUE)
which_extra_mask_1 <- grep("mask_1", lf)
to_remove <- c(sort(c(which_extra_mask_1, which_extra_mask_1 - 1, which_extra_mask_1 - 2)), grep("mask_2", lf))
lf_subset <- lf[-to_remove]
# Load images and masks
img <- purrr::map(lf_subset, ~ imager::load.image(.))
mask <- purrr::map(lf_subset, ~ imager::load.image(.))
# Split the data
split_tmp <- split_data(n = length(img), frac_test = test_split, frac_val = validation_split, seed = 123)
test_index <- split_tmp$index_test
val_index <- split_tmp$index_val
train_index <- split_tmp$index_train
# Convert images and masks to 4D arrays for keras
x_train <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = train_index)
y_train <- imagesToKerasInput(images = mask, type = "mask", subset = train_index)
x_test <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = test_index)
y_test <- imagesToKerasInput(images = mask, type = "mask", subset = test_index)
x_val <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = val_index)
y_val <- imagesToKerasInput(images = mask, type = "mask", subset = val_index)
# Create the model architecture
model <- u_net(net_w = dimensions[1], net_h = dimensions[2], grayscale = TRUE, n_class = 1, filters = 32)
# Model compilation
model %>% compile(optimizer = optimizer_adam(), loss = loss_binary_crossentropy, metrics = list(metric_binary_accuracy))
# Set up the sampling generator
sampling_generator <- function(x_data, y_data, batch_size) {
function() {
rows <- sample(1:length(x_data), batch_size, replace = TRUE)
list(x_data[rows,,,, drop = FALSE], y_data[rows,,,, drop = FALSE])
}
}
# Train the model
history <- model %>% fit(x = sampling_generator(x_train, y_train, batch_size = batch_size),
epochs = epochs,
steps_per_epoch = round(length(x_train) / batch_size),
validation_data = list(x_val, y_val))
# Model evaluation
scores <- model %>% evaluate(x_test, y_test, verbose = 0)
print(scores)
# Perform image segmentation on test images
out <- imageSegmentation(model, x = x_test, threshold = threshold)
# Return the results or do further processing as needed
return(out)
}
# Example usage
dataset_path <- "C:/Users/User/Downloads/Dataset_BUSI_with_GT"
detection_results <- perform_object_detection(dataset_path)
warnings()
perform_object_detection <- function(dataset_path, dimensions = c(256, 256), test_split = 0.2, validation_split = 0.1, epochs = 20, batch_size = 4, threshold = 0.8) {
all_files <- list.files(dataset_path, full.names = TRUE, recursive = TRUE)
img <- vector("list", length = length(all_files))
for (i in seq_along(all_files)) {
img[i] <- imager::load.image(all_files[i])
if (i %% batch_size == 0) {
message("Loaded ", i, " images out of ", length(all_files))
}
}
# Remove images with multiple masks
lf <- list.files(dataset_path, full.names = TRUE, recursive = TRUE)
which_extra_mask_1 <- grep("mask_1", lf)
to_remove <- c(sort(c(which_extra_mask_1, which_extra_mask_1 - 1, which_extra_mask_1 - 2)), grep("mask_2", lf))
lf_subset <- lf[-to_remove]
# Load images and masks
img <- purrr::map(lf_subset, ~ imager::load.image(.))
mask <- purrr::map(lf_subset, ~ imager::load.image(.))
# Split the data
split_tmp <- split_data(n = length(img), frac_test = test_split, frac_val = validation_split, seed = 123)
test_index <- split_tmp$index_test
val_index <- split_tmp$index_val
train_index <- split_tmp$index_train
# Convert images and masks to 4D arrays for keras
x_train <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = train_index)
y_train <- imagesToKerasInput(images = mask, type = "mask", subset = train_index)
x_test <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = test_index)
y_test <- imagesToKerasInput(images = mask, type = "mask", subset = test_index)
x_val <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = val_index)
y_val <- imagesToKerasInput(images = mask, type = "mask", subset = val_index)
# Create the model architecture
model <- u_net(net_w = dimensions[1], net_h = dimensions[2], grayscale = TRUE, n_class = 1, filters = 32)
# Model compilation
model %>% compile(optimizer = optimizer_adam(), loss = loss_binary_crossentropy, metrics = list(metric_binary_accuracy))
# Set up the sampling generator
sampling_generator <- function(x_data, y_data, batch_size) {
function() {
rows <- sample(1:length(x_data), batch_size, replace = TRUE)
list(x_data[rows,,,, drop = FALSE], y_data[rows,,,, drop = FALSE])
}
}
# Train the model
history <- model %>% fit(x = sampling_generator(x_train, y_train, batch_size = batch_size),
epochs = epochs,
steps_per_epoch = round(length(x_train) / batch_size),
validation_data = list(x_val, y_val))
# Model evaluation
scores <- model %>% evaluate(x_test, y_test, verbose = 0)
print(scores)
# Perform image segmentation on test images
out <- imageSegmentation(model, x = x_test, threshold = threshold)
# Return the results or do further processing as needed
return(out)
}
# Example usage
dataset_path <- "C:/Users/User/Downloads/Dataset_BUSI_with_GT"
detection_results <- perform_object_detection(dataset_path)
perform_object_detection <- function(dataset_path, subset_size = NULL, dimensions = c(256, 256), test_split = 0.2, validation_split = 0.1, epochs = 20, batch_size = 8, threshold = 0.8) {
all_files <- list.files(dataset_path, full.names = TRUE, recursive = TRUE)
# Take a subset of files for demonstration
if (!is.null(subset_size)) {
all_files <- sample(all_files, size = subset_size)
}
img <- vector("list", length = length(all_files))
for (i in seq_along(all_files)) {
img[i] <- imager::load.image(all_files[i])
if (i %% batch_size == 0) {
message("Loaded ", i, " images out of ", length(all_files))
}
}
# Remove images with multiple masks
lf <- list.files(dataset_path, full.names = TRUE, recursive = TRUE)
which_extra_mask_1 <- grep("mask_1", lf)
to_remove <- c(sort(c(which_extra_mask_1, which_extra_mask_1 - 1, which_extra_mask_1 - 2)), grep("mask_2", lf))
lf_subset <- lf[-to_remove]
# Load images and masks
img <- purrr::map(lf_subset, ~ imager::load.image(.))
mask <- purrr::map(lf_subset, ~ imager::load.image(.))
# Split the data
split_tmp <- split_data(n = length(img), frac_test = test_split, frac_val = validation_split, seed = 123)
test_index <- split_tmp$index_test
val_index <- split_tmp$index_val
train_index <- split_tmp$index_train
# Convert images and masks to 4D arrays for keras
x_train <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = train_index)
y_train <- imagesToKerasInput(images = mask, type = "mask", subset = train_index)
x_test <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = test_index)
y_test <- imagesToKerasInput(images = mask, type = "mask", subset = test_index)
x_val <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = val_index)
y_val <- imagesToKerasInput(images = mask, type = "mask", subset = val_index)
# Create the model architecture
model <- u_net(net_w = dimensions[1], net_h = dimensions[2], grayscale = TRUE, n_class = 1, filters = 32)
# Model compilation
model %>% compile(optimizer = optimizer_adam(), loss = loss_binary_crossentropy, metrics = list(metric_binary_accuracy))
# Set up the sampling generator
sampling_generator <- function(x_data, y_data, batch_size) {
function() {
rows <- sample(1:length(x_data), batch_size, replace = TRUE)
list(x_data[rows,,,, drop = FALSE], y_data[rows,,,, drop = FALSE])
}
}
# Train the model
history <- model %>% fit(x = sampling_generator(x_train, y_train, batch_size = batch_size),
epochs = epochs,
steps_per_epoch = round(length(x_train) / batch_size),
validation_data = list(x_val, y_val))
# Model evaluation
scores <- model %>% evaluate(x_test, y_test, verbose = 0)
print(scores)
# Perform image segmentation on test images
out <- imageSegmentation(model, x = x_test, threshold = threshold)
# Return the results or do further processing as needed
return(out)
}
# Example usage
dataset_path <- "C:/Users/User/Downloads/Dataset_BUSI_with_GT"
detection_results <- perform_object_detection(dataset_path,20)
# Install and load the simstudy package
install.packages("simstudy")
library(simstudy)
# Set seed for reproducibility
set.seed(123)
# Define a data definition
def <- defData(varname = "Age", formula = 30, variance = 10)
def <- defData(def, varname = "BloodPressure", formula = "30 + Age + rnorm(n, 0, 5)")
def <- defData(def, varname = "Cholesterol", formula = "200 + 0.2 * Age + rnorm(n, 0, 10)")
def <- defData(def, varname = "BMI", formula = "25 + rnorm(n, 0, 3)")
# Generate synthetic data
n <- 1000
synthetic_data <- genData(n, def)
# View the synthetic data
head(synthetic_data)
devtools::load_all(".")
rm(list = c("perform_object_detection"))
devtools::load_all(".")
library(EducationalHealthcareRPackage)
devtools::load_all(".")
library(EducationalHealthcareRPackage)
library(keras)
install.packages("keras")
library(keras)
library('keras')
devtools::load_all(".")
devtools::load_all(".")
reticulate::py_install("tensorflow")
reticulate::py_module_available("tensorflow")
reticulate::py_config()
# Install numpy
reticulate::py_install("numpy")
# Install tensorflow
reticulate::py_install("tensorflow")
library(keras)
remotes::install_github("rstudio/keras")
reticulate::py_module_available("tensorflow")
