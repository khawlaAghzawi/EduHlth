model %>% compile(optimizer = optimizer_adam(), loss = loss_binary_crossentropy, metrics = list(metric_binary_accuracy))
# Set up the sampling generator
sampling_generator <- function(x_data, y_data, batch_size) {
function() {
rows <- sample(1:nrow(x_data), batch_size, replace = TRUE)
list(x_data[rows,,,, drop = FALSE], y_data[rows,,,, drop = FALSE])
}
}
# Train the model
history <- model %>% fit(x = sampling_generator(x_train, y_train, batch_size = batch_size),
epochs = epochs,
steps_per_epoch = round(nrow(x_train) / batch_size),
validation_data = list(x_val, y_val))
# Model evaluation
scores <- model %>% evaluate(x_test, y_test, verbose = 0)
print(scores)
# Perform image segmentation on test images
out <- imageSegmentation(model, x = x_test, threshold = threshold)
# Return the results or do further processing as needed
return(out)
}
# Example usage
dataset_path <- "C:/Users/User/Downloads/Dataset_BUSI_with_GT"
detection_results <- perform_object_detection(dataset_path)
dataset_path <- "D:/Testing/imageSegmentation/kaggle/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT"
all_files <- list.files(dataset_path, full.names = TRUE, recursive = TRUE)
# Set the batch size
batch_size <- 50
# Use a loop to load images in batches
img <- vector("list", length = length(all_files))
for (i in seq_along(all_files)) {
img[i] <- load.image(all_files[i])
if (i %% batch_size == 0) {
message("Loaded ", i, " images out of ", length(all_files))
}
}
perform_object_detection <- function(dataset_path, dimensions = c(256, 256), test_split = 0.2, validation_split = 0.1, epochs = 20, batch_size = 8, threshold = 0.8) {
all_files <- list.files(dataset_path, full.names = TRUE, recursive = TRUE)
img <- vector("list", length = length(all_files))
for (i in seq_along(all_files)) {
img[i] <- imager::load.image(all_files[i])
if (i %% batch_size == 0) {
message("Loaded ", i, " images out of ", length(all_files))
}
}
# Remove images with multiple masks
lf <- list.files(wd_resized, full.names = TRUE, recursive = TRUE)
which_extra_mask_1 <- grep("mask_1", lf)
to_remove <- c(sort(c(which_extra_mask_1, which_extra_mask_1 - 1, which_extra_mask_1 - 2)), grep("mask_2", lf))
lf_subset <- lf[-to_remove]
# Load images and masks
img <- loadImages(fileNames = lf_subset, pattern = "mask", patternInclude = FALSE)
mask <- loadImages(fileNames = lf_subset, pattern = "mask", patternInclude = TRUE)
# Split the data
split_tmp <- split_data(n = nrow(img$info), frac_test = test_split, frac_val = validation_split, seed = 123)
test_index <- split_tmp$index_test
val_index <- split_tmp$index_val
train_index <- split_tmp$index_train
# Convert images and masks to 4D arrays for keras
x_train <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = train_index)
y_train <- imagesToKerasInput(images = mask, type = "mask", subset = train_index)
x_test <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = test_index)
y_test <- imagesToKerasInput(images = mask, type = "mask", subset = test_index)
x_val <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = val_index)
y_val <- imagesToKerasInput(images = mask, type = "mask", subset = val_index)
# Create the model architecture
model <- u_net(net_w = dimensions[1], net_h = dimensions[2], grayscale = TRUE, n_class = 1, filters = 32)
# Model compilation
model %>% compile(optimizer = optimizer_adam(), loss = loss_binary_crossentropy, metrics = list(metric_binary_accuracy))
# Set up the sampling generator
sampling_generator <- function(x_data, y_data, batch_size) {
function() {
rows <- sample(1:nrow(x_data), batch_size, replace = TRUE)
list(x_data[rows,,,, drop = FALSE], y_data[rows,,,, drop = FALSE])
}
}
# Train the model
history <- model %>% fit(x = sampling_generator(x_train, y_train, batch_size = batch_size),
epochs = epochs,
steps_per_epoch = round(nrow(x_train) / batch_size),
validation_data = list(x_val, y_val))
# Model evaluation
scores <- model %>% evaluate(x_test, y_test, verbose = 0)
print(scores)
# Perform image segmentation on test images
out <- imageSegmentation(model, x = x_test, threshold = threshold)
# Return the results or do further processing as needed
return(out)
}
# Example usage
dataset_path <- "C:/Users/User/Downloads/Dataset_BUSI_with_GT"
detection_results <- perform_object_detection(dataset_path)
library(imager)
library(purrr)
?load.image
perform_object_detection <- function(dataset_path, dimensions = c(256, 256), test_split = 0.2, validation_split = 0.1, epochs = 20, batch_size = 8, threshold = 0.8) {
all_files <- list.files(dataset_path, full.names = TRUE, recursive = TRUE)
img <- vector("list", length = length(all_files))
for (i in seq_along(all_files)) {
img[i] <- imager::load.image(all_files[i])
if (i %% batch_size == 0) {
message("Loaded ", i, " images out of ", length(all_files))
}
}
# Remove images with multiple masks
lf <- list.files(wd_resized, full.names = TRUE, recursive = TRUE)
which_extra_mask_1 <- grep("mask_1", lf)
to_remove <- c(sort(c(which_extra_mask_1, which_extra_mask_1 - 1, which_extra_mask_1 - 2)), grep("mask_2", lf))
lf_subset <- lf[-to_remove]
# Load images and masks
img <- loadImages(fileNames = lf_subset, pattern = "mask", patternInclude = FALSE)
mask <- loadImages(fileNames = lf_subset, pattern = "mask", patternInclude = TRUE)
# Split the data
split_tmp <- split_data(n = nrow(img$info), frac_test = test_split, frac_val = validation_split, seed = 123)
test_index <- split_tmp$index_test
val_index <- split_tmp$index_val
train_index <- split_tmp$index_train
# Convert images and masks to 4D arrays for keras
x_train <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = train_index)
y_train <- imagesToKerasInput(images = mask, type = "mask", subset = train_index)
x_test <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = test_index)
y_test <- imagesToKerasInput(images = mask, type = "mask", subset = test_index)
x_val <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = val_index)
y_val <- imagesToKerasInput(images = mask, type = "mask", subset = val_index)
# Create the model architecture
model <- u_net(net_w = dimensions[1], net_h = dimensions[2], grayscale = TRUE, n_class = 1, filters = 32)
# Model compilation
model %>% compile(optimizer = optimizer_adam(), loss = loss_binary_crossentropy, metrics = list(metric_binary_accuracy))
# Set up the sampling generator
sampling_generator <- function(x_data, y_data, batch_size) {
function() {
rows <- sample(1:nrow(x_data), batch_size, replace = TRUE)
list(x_data[rows,,,, drop = FALSE], y_data[rows,,,, drop = FALSE])
}
}
# Train the model
history <- model %>% fit(x = sampling_generator(x_train, y_train, batch_size = batch_size),
epochs = epochs,
steps_per_epoch = round(nrow(x_train) / batch_size),
validation_data = list(x_val, y_val))
# Model evaluation
scores <- model %>% evaluate(x_test, y_test, verbose = 0)
print(scores)
# Perform image segmentation on test images
out <- imageSegmentation(model, x = x_test, threshold = threshold)
# Return the results or do further processing as needed
return(out)
}
# Example usage
dataset_path <- "C:/Users/User/Downloads/Dataset_BUSI_with_GT"
detection_results <- perform_object_detection(dataset_path)
perform_object_detection <- function(dataset_path, dimensions = c(256, 256), test_split = 0.2, validation_split = 0.1, epochs = 20, batch_size = 8, threshold = 0.8) {
all_files <- list.files(dataset_path, full.names = TRUE, recursive = TRUE)
img <- vector("list", length = length(all_files))
for (i in seq_along(all_files)) {
img[i] <- imager::load.image(all_files[i])
if (i %% batch_size == 0) {
message("Loaded ", i, " images out of ", length(all_files))
}
}
# Remove images with multiple masks
lf <- list.files(wd_resized, full.names = TRUE, recursive = TRUE)
which_extra_mask_1 <- grep("mask_1", lf)
to_remove <- c(sort(c(which_extra_mask_1, which_extra_mask_1 - 1, which_extra_mask_1 - 2)), grep("mask_2", lf))
lf_subset <- lf[-to_remove]
# Load images and masks
img <- imager::load.image(fileNames = lf_subset, pattern = "mask", patternInclude = FALSE)
mask <- imager::load.image(fileNames = lf_subset, pattern = "mask", patternInclude = TRUE)
# Split the data
split_tmp <- split_data(n = nrow(img$info), frac_test = test_split, frac_val = validation_split, seed = 123)
test_index <- split_tmp$index_test
val_index <- split_tmp$index_val
train_index <- split_tmp$index_train
# Convert images and masks to 4D arrays for keras
x_train <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = train_index)
y_train <- imagesToKerasInput(images = mask, type = "mask", subset = train_index)
x_test <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = test_index)
y_test <- imagesToKerasInput(images = mask, type = "mask", subset = test_index)
x_val <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = val_index)
y_val <- imagesToKerasInput(images = mask, type = "mask", subset = val_index)
# Create the model architecture
model <- u_net(net_w = dimensions[1], net_h = dimensions[2], grayscale = TRUE, n_class = 1, filters = 32)
# Model compilation
model %>% compile(optimizer = optimizer_adam(), loss = loss_binary_crossentropy, metrics = list(metric_binary_accuracy))
# Set up the sampling generator
sampling_generator <- function(x_data, y_data, batch_size) {
function() {
rows <- sample(1:nrow(x_data), batch_size, replace = TRUE)
list(x_data[rows,,,, drop = FALSE], y_data[rows,,,, drop = FALSE])
}
}
# Train the model
history <- model %>% fit(x = sampling_generator(x_train, y_train, batch_size = batch_size),
epochs = epochs,
steps_per_epoch = round(nrow(x_train) / batch_size),
validation_data = list(x_val, y_val))
# Model evaluation
scores <- model %>% evaluate(x_test, y_test, verbose = 0)
print(scores)
# Perform image segmentation on test images
out <- imageSegmentation(model, x = x_test, threshold = threshold)
# Return the results or do further processing as needed
return(out)
}
# Example usage
dataset_path <- "C:/Users/User/Downloads/Dataset_BUSI_with_GT"
detection_results <- perform_object_detection(dataset_path)
perform_object_detection <- function(dataset_path, dimensions = c(256, 256), test_split = 0.2, validation_split = 0.1, epochs = 20, batch_size = 8, threshold = 0.8) {
all_files <- list.files(dataset_path, full.names = TRUE, recursive = TRUE)
img <- vector("list", length = length(all_files))
for (i in seq_along(all_files)) {
img[i] <- imager::load.image(all_files[i])
if (i %% batch_size == 0) {
message("Loaded ", i, " images out of ", length(all_files))
}
}
# Remove images with multiple masks
lf <- list.files(dataset_path, full.names = TRUE, recursive = TRUE)
which_extra_mask_1 <- grep("mask_1", lf)
to_remove <- c(sort(c(which_extra_mask_1, which_extra_mask_1 - 1, which_extra_mask_1 - 2)), grep("mask_2", lf))
lf_subset <- lf[-to_remove]
# Load images and masks
img <- purrr::map(lf_subset, ~ imager::load.image(.))
mask <- purrr::map(lf_subset, ~ imager::load.image(.))
# Split the data
split_tmp <- split_data(n = length(img), frac_test = test_split, frac_val = validation_split, seed = 123)
test_index <- split_tmp$index_test
val_index <- split_tmp$index_val
train_index <- split_tmp$index_train
# Convert images and masks to 4D arrays for keras
x_train <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = train_index)
y_train <- imagesToKerasInput(images = mask, type = "mask", subset = train_index)
x_test <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = test_index)
y_test <- imagesToKerasInput(images = mask, type = "mask", subset = test_index)
x_val <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = val_index)
y_val <- imagesToKerasInput(images = mask, type = "mask", subset = val_index)
# Create the model architecture
model <- u_net(net_w = dimensions[1], net_h = dimensions[2], grayscale = TRUE, n_class = 1, filters = 32)
# Model compilation
model %>% compile(optimizer = optimizer_adam(), loss = loss_binary_crossentropy, metrics = list(metric_binary_accuracy))
# Set up the sampling generator
sampling_generator <- function(x_data, y_data, batch_size) {
function() {
rows <- sample(1:length(x_data), batch_size, replace = TRUE)
list(x_data[rows,,,, drop = FALSE], y_data[rows,,,, drop = FALSE])
}
}
# Train the model
history <- model %>% fit(x = sampling_generator(x_train, y_train, batch_size = batch_size),
epochs = epochs,
steps_per_epoch = round(length(x_train) / batch_size),
validation_data = list(x_val, y_val))
# Model evaluation
scores <- model %>% evaluate(x_test, y_test, verbose = 0)
print(scores)
# Perform image segmentation on test images
out <- imageSegmentation(model, x = x_test, threshold = threshold)
# Return the results or do further processing as needed
return(out)
}
# Example usage
dataset_path <- "C:/Users/User/Downloads/Dataset_BUSI_with_GT"
detection_results <- perform_object_detection(dataset_path)
warnings()
perform_object_detection <- function(dataset_path, dimensions = c(256, 256), test_split = 0.2, validation_split = 0.1, epochs = 20, batch_size = 4, threshold = 0.8) {
all_files <- list.files(dataset_path, full.names = TRUE, recursive = TRUE)
img <- vector("list", length = length(all_files))
for (i in seq_along(all_files)) {
img[i] <- imager::load.image(all_files[i])
if (i %% batch_size == 0) {
message("Loaded ", i, " images out of ", length(all_files))
}
}
# Remove images with multiple masks
lf <- list.files(dataset_path, full.names = TRUE, recursive = TRUE)
which_extra_mask_1 <- grep("mask_1", lf)
to_remove <- c(sort(c(which_extra_mask_1, which_extra_mask_1 - 1, which_extra_mask_1 - 2)), grep("mask_2", lf))
lf_subset <- lf[-to_remove]
# Load images and masks
img <- purrr::map(lf_subset, ~ imager::load.image(.))
mask <- purrr::map(lf_subset, ~ imager::load.image(.))
# Split the data
split_tmp <- split_data(n = length(img), frac_test = test_split, frac_val = validation_split, seed = 123)
test_index <- split_tmp$index_test
val_index <- split_tmp$index_val
train_index <- split_tmp$index_train
# Convert images and masks to 4D arrays for keras
x_train <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = train_index)
y_train <- imagesToKerasInput(images = mask, type = "mask", subset = train_index)
x_test <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = test_index)
y_test <- imagesToKerasInput(images = mask, type = "mask", subset = test_index)
x_val <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = val_index)
y_val <- imagesToKerasInput(images = mask, type = "mask", subset = val_index)
# Create the model architecture
model <- u_net(net_w = dimensions[1], net_h = dimensions[2], grayscale = TRUE, n_class = 1, filters = 32)
# Model compilation
model %>% compile(optimizer = optimizer_adam(), loss = loss_binary_crossentropy, metrics = list(metric_binary_accuracy))
# Set up the sampling generator
sampling_generator <- function(x_data, y_data, batch_size) {
function() {
rows <- sample(1:length(x_data), batch_size, replace = TRUE)
list(x_data[rows,,,, drop = FALSE], y_data[rows,,,, drop = FALSE])
}
}
# Train the model
history <- model %>% fit(x = sampling_generator(x_train, y_train, batch_size = batch_size),
epochs = epochs,
steps_per_epoch = round(length(x_train) / batch_size),
validation_data = list(x_val, y_val))
# Model evaluation
scores <- model %>% evaluate(x_test, y_test, verbose = 0)
print(scores)
# Perform image segmentation on test images
out <- imageSegmentation(model, x = x_test, threshold = threshold)
# Return the results or do further processing as needed
return(out)
}
# Example usage
dataset_path <- "C:/Users/User/Downloads/Dataset_BUSI_with_GT"
detection_results <- perform_object_detection(dataset_path)
perform_object_detection <- function(dataset_path, subset_size = NULL, dimensions = c(256, 256), test_split = 0.2, validation_split = 0.1, epochs = 20, batch_size = 8, threshold = 0.8) {
all_files <- list.files(dataset_path, full.names = TRUE, recursive = TRUE)
# Take a subset of files for demonstration
if (!is.null(subset_size)) {
all_files <- sample(all_files, size = subset_size)
}
img <- vector("list", length = length(all_files))
for (i in seq_along(all_files)) {
img[i] <- imager::load.image(all_files[i])
if (i %% batch_size == 0) {
message("Loaded ", i, " images out of ", length(all_files))
}
}
# Remove images with multiple masks
lf <- list.files(dataset_path, full.names = TRUE, recursive = TRUE)
which_extra_mask_1 <- grep("mask_1", lf)
to_remove <- c(sort(c(which_extra_mask_1, which_extra_mask_1 - 1, which_extra_mask_1 - 2)), grep("mask_2", lf))
lf_subset <- lf[-to_remove]
# Load images and masks
img <- purrr::map(lf_subset, ~ imager::load.image(.))
mask <- purrr::map(lf_subset, ~ imager::load.image(.))
# Split the data
split_tmp <- split_data(n = length(img), frac_test = test_split, frac_val = validation_split, seed = 123)
test_index <- split_tmp$index_test
val_index <- split_tmp$index_val
train_index <- split_tmp$index_train
# Convert images and masks to 4D arrays for keras
x_train <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = train_index)
y_train <- imagesToKerasInput(images = mask, type = "mask", subset = train_index)
x_test <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = test_index)
y_test <- imagesToKerasInput(images = mask, type = "mask", subset = test_index)
x_val <- imagesToKerasInput(img, type = "image", grayscale = TRUE, subset = val_index)
y_val <- imagesToKerasInput(images = mask, type = "mask", subset = val_index)
# Create the model architecture
model <- u_net(net_w = dimensions[1], net_h = dimensions[2], grayscale = TRUE, n_class = 1, filters = 32)
# Model compilation
model %>% compile(optimizer = optimizer_adam(), loss = loss_binary_crossentropy, metrics = list(metric_binary_accuracy))
# Set up the sampling generator
sampling_generator <- function(x_data, y_data, batch_size) {
function() {
rows <- sample(1:length(x_data), batch_size, replace = TRUE)
list(x_data[rows,,,, drop = FALSE], y_data[rows,,,, drop = FALSE])
}
}
# Train the model
history <- model %>% fit(x = sampling_generator(x_train, y_train, batch_size = batch_size),
epochs = epochs,
steps_per_epoch = round(length(x_train) / batch_size),
validation_data = list(x_val, y_val))
# Model evaluation
scores <- model %>% evaluate(x_test, y_test, verbose = 0)
print(scores)
# Perform image segmentation on test images
out <- imageSegmentation(model, x = x_test, threshold = threshold)
# Return the results or do further processing as needed
return(out)
}
# Example usage
dataset_path <- "C:/Users/User/Downloads/Dataset_BUSI_with_GT"
detection_results <- perform_object_detection(dataset_path,20)
# Install and load the simstudy package
install.packages("simstudy")
library(simstudy)
# Set seed for reproducibility
set.seed(123)
# Define a data definition
def <- defData(varname = "Age", formula = 30, variance = 10)
def <- defData(def, varname = "BloodPressure", formula = "30 + Age + rnorm(n, 0, 5)")
def <- defData(def, varname = "Cholesterol", formula = "200 + 0.2 * Age + rnorm(n, 0, 10)")
def <- defData(def, varname = "BMI", formula = "25 + rnorm(n, 0, 3)")
# Generate synthetic data
n <- 1000
synthetic_data <- genData(n, def)
# View the synthetic data
head(synthetic_data)
devtools::load_all(".")
rm(list = c("perform_object_detection"))
devtools::load_all(".")
library(EducationalHealthcareRPackage)
devtools::load_all(".")
library(EducationalHealthcareRPackage)
library(keras)
install.packages("keras")
library(keras)
library('keras')
devtools::load_all(".")
devtools::load_all(".")
reticulate::py_install("tensorflow")
reticulate::py_module_available("tensorflow")
reticulate::py_config()
# Install numpy
reticulate::py_install("numpy")
# Install tensorflow
reticulate::py_install("tensorflow")
library(keras)
remotes::install_github("rstudio/keras")
reticulate::py_module_available("tensorflow")
devtools::load_all(".")
library(EducationalHealthcareRPackage)
library(EducationalHealthcareRPackage)
dataset_path <- "C:/Users/User/Downloads/Dataset_BUSI_with_GT"
detection_results <- perform_object_detection(dataset_path, 20)
library(EducationalHealthcareRPackage)
#' Preprocess Predictive Models Data
#'
#' This function preprocesses predictive models data.
#'
#' @param predictive_models_data A dataframe containing predictive models data.
#' @return A preprocessed dataframe.
#' @export
#'
preprocess_predictive_models_data <- function(predictive_models_data) {
# Placeholder for preprocessing steps
# Replace this with your actual preprocessing code
message("Preprocessing predictive models data...")
# Example: preprocessed_data <- your_preprocessing_function(predictive_models_data)
# Return the preprocessed dataframe
return(preprocessed_data)
}
# Generate synthetic data
set.seed(456)
synthetic_data <- data.frame(
feature1 = rnorm(100),
feature2 = rnorm(100),
outcome = sample(c("Disease_A", "Disease_B"), 100, replace = TRUE)
)
# Build predictive model
model <- build_predictive_model(synthetic_data)
# Make predictions on the test set
predictions <- predict(model, newdata = test_data)
library(caret)
library(randomForest)
# Function to build predictive model
build_predictive_model <- function(data) {
# Assuming 'outcome' is the outcome variable, and other variables are features
formula <- outcome ~ .
# Split the data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(data$outcome, p = 0.8, list = FALSE)
train_data <- data[trainIndex, ]
test_data <- data[-trainIndex, ]
# Train a random forest model
model <- randomForest(formula, data = train_data)
# Return the trained model
return(model)
}
# Generate synthetic data
set.seed(456)
synthetic_data <- data.frame(
feature1 = rnorm(100),
feature2 = rnorm(100),
outcome = sample(c("Disease_A", "Disease_B"), 100, replace = TRUE)
)
# Build predictive model
trained_model <- build_predictive_model(synthetic_data)
# Generate synthetic data with binary outcome
set.seed(456)
synthetic_data <- data.frame(
feature1 = rnorm(100),
feature2 = rnorm(100),
outcome = as.factor(sample(c("Disease_A", "Disease_B"), 100, replace = TRUE))
)
# Build predictive model
trained_model <- build_predictive_model(synthetic_data)
trained_model
library(devtools)
install_github("khawlaAghzawi/EduactionalHealthcareRPackage")
library(devtools)
install_github("khawlaAghzawi/EduactionalHealthcareRPackage")
install.packages("glue")
install.packages("glue")
install.packages("EducationalHealthcareRPackage", INSTALL_opts = c("--install-tests", "--debug"))
remotes::install_github("khawlaAghzawi/EduactionalHealthcareRPackage")
library(devtools)
install_github("khawlaAghzawi/EduactionalHealthcareRPackage")
library(EducationalHealthcareRPackage)
?analyze_clinical_trial_data
?analyze_ehr_data
?visualize_ehr_data
install.packages("shiny")
install.packages("shiny")
?visualize_ehr_data
library(EducationalHealthcareRPackage)
?visualize_ehr_data
library(shiny)
?visualize_ehr_data
?load_medical_imaging_data
?
analyze_ehr_data
?build_predictive_model
?preprocess_medical_imaging_data
?visualize_clinical_trial_data
